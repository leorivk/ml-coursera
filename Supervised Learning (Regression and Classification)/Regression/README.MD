# 회귀 Regression

주어진 입력 변수와 출력 변수 간의 관계를 모델링하여 연속적인 출력값을 예측하는 분석 방법

### 예시

<img src="../image/Regression Example.png" width=80%>

</br>

## 선형 회귀 (Linear Regression)

주어진 입력 변수와 출력 변수 간의 선형 관계를 모델링

### 예시

주어진 주택 크기와 주택 가격 데이터를 통해 선형 관계를 모델링하여 원하는 주택 크기에 해당하는 주택 가격을 예측 

<img src="../image/Linear Regression Example 1.png" width=80%><br/>

<img src="../image/Linear Regression Example 2.png" width=80%><br/>

<img src="../image/Terminology Example.png" width=80%><br/>

</br>

### 표기법 (Terminology)

<img src="../image/Terminology.png" width=80%><br/>

</br>

### 선형 회귀를 통한 지도 학습 과정

<img src="../image/선형 회귀 지도 학습 과정.png" width=80%></br>

1. 입력값(Features)과 출력값(Targets)이 포함된 훈련 예제(Training Set)를 통해 모델 생성

2. 모델은 입력값을 받아 예측값($ŷ$)을 출력

</br>

### 모델 f를 표현하는 방법

<img src="../image/모델 f 표현.png" width=80%></br>

### 비용 함수
: 모델의 예측값과 실제 관찰값(또는 레이블) 간의 차이 또는 오차($y$, $ŷ$ 간의 차이)를 측정하는 함수

</br>

$f(x) = wx+b$ 의 $w$, $b$ : 모델의 매개 변수(계수, 가중치)

> 매개 변수 : 모델의 개선을 위해 조정할 수 있는 변수

- 직선이 훈련 데이터에 잘 맞도록 $w$, $b$의 값 조정

- 비용 함수를 통해 직선이 훈련 데이터에 얼마나 잘 맞는지 측정

- 선형 회귀의 경우 일반적으로 제곱 오차 비용 함수 사용

#### 제곱 오차 비용 함수
<img src="../image/제곱 오차 비용 함수.png" width=80%>

</br>

<img src="../image/비용 함수 J 그래프.png" width=80%></br>

***비용 함수 $J$의 최소값을 구하는 것이 선형 회귀의 목표***

</br>

### 경사 하강법
: 비용 함수 $J$의 (국소)최소값을 구하기 위한 알고리즘

1. 모델의 파라미터 초기화

2. 알고리즘이 수렴할 때까지 비용 함수를 최소화하는 방향으로 비용 함수의 기울기(경사)를 따라 파라미터 값 조정


</br>

<img src="../image/경사 하강법.png" width=80%></br>
- $w$, $b$를 동시에 조정

#### $J$의 도함수 (편도함수)
<img src="../image/J의 도함수.png" width=80%></br>
- 학습률은 항상 양수

- 기울기가 양수일 경우 매 업데이트마다 파라미터 감소 ($w - 양수 × 양수$)

- 음수일 경우 매 업데이트마다 파라미터 증가 ($w - 양수 × 음수$)

- 위 과정 반복하며 $J$의 최소값에 근접 (수렴)


#### 학습률 (alpha)
: 각 반복마다 모델 파라미터를 얼마나 크게 조정할 지 결정

- 학습률이 너무 작으면?
    - 매우 많은 조정 단계가 필요하므로 학습 속도 매우 느려짐

- 너무 크면?
    - 수렴하지 못하고 발산할 가능성 존재

#### 고정된 학습률로도 (국소)최소값에 도달할 수 있는 이유
<img src="../image/고정된 학습률.png" width=80%></br>
- 최소값에 가까워질수록 기울기(도함수)가 작아지기 때문





